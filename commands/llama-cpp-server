#!/usr/bin/env bash

function usage() {
    echo "Usage: llama-cpp-server -m <model> [-o <offload_num>] [-c <ctx_size>] [other_args]"
    exit 1
}

function llama-cpp-server() {
    source "$DOROTHY/sources/bash.bash"

    # Default values
    MODEL=""
    OFFLOAD_NUM=100
    CTX_SIZE=0

    # Parse command line options
    while getopts ":m:o:c:" opt; do
        case $opt in
            m) MODEL="$OPTARG" ;;
            o) OFFLOAD_NUM="$OPTARG" ;;
            c) CTX_SIZE="$OPTARG" ;;
            \?) echo "Invalid option -$OPTARG" >&2; usage ;;
            :) echo "Option -$OPTARG requires an argument." >&2; usage ;;
        esac
    done
    shift $((OPTIND -1))

    OTHERARGS="$@"

    if [ -z "$MODEL" ]; then
        usage
    fi

    set -x

    #docker run --gpus all --rm --name llama.cpp -p 8080:8080 \
    #-v /etc/ssl/certs:/etc/ssl/certs:ro \
    #-v ~/.cache/llama.cpp:/root/.cache/llama.cpp \
    #ghcr.io/ggml-org/llama.cpp:full-cuda \
    #-s \
    
    ARGS="--ctx-size $CTX_SIZE \
    --repeat-penalty 1.3 \
    --jinja \
    -fa \
    -hf "$MODEL" \
    --host 0.0.0.0 \
    -ngl $OFFLOAD_NUM \
    $OTHERARGS"

    setsid env GGML_CUDA_ENABLE_UNIFIED_MEMORY=1 llama-server $ARGS &
    pid=$!
    trap "kill -- -$pid" SIGINT SIGTERM EXIT
    wait $pid
}

# fire if invoked standalone
if test "$0" = "${BASH_SOURCE[0]}"; then
    llama-cpp-server "$@"
fi